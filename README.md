# adt_project

The 'adt_project' project was generated by using the default-python template.

## Getting started

0. Install UV: https://docs.astral.sh/uv/getting-started/installation/

1. Install the Databricks CLI from https://docs.databricks.com/dev-tools/cli/databricks-cli.html

2. Authenticate to your Databricks workspace, if you have not done so already:
    ```
    $ databricks configure
    ```

3. To deploy a development copy of this project, type:
    ```
    $ databricks bundle deploy --target dev
    ```
    (Note that "dev" is the default target, so the `--target` parameter
    is optional here.)

    This deploys everything that's defined for this project.
    For example, the default template would deploy a job called
    `[dev yourname] adt_project_job` to your workspace.
    You can find that job by opening your workpace and clicking on **Workflows**.

4. Similarly, to deploy a production copy, type:
   ```
   $ databricks bundle deploy --target prod
   ```

   Note that the default job from the template has a schedule that runs every day
   (defined in resources/adt_project.job.yml). The schedule
   is paused when deploying in development mode (see
   https://docs.databricks.com/dev-tools/bundles/deployment-modes.html).

5. To run a job or pipeline, use the "run" command:
   ```
   $ databricks bundle run
   ```
6. Optionally, install developer tools such as the Databricks extension for Visual Studio Code from
   https://docs.databricks.com/dev-tools/vscode-ext.html.

7. For documentation on the Databricks asset bundles format used
   for this project, and for CI/CD configuration, see
   https://docs.databricks.com/dev-tools/bundles/index.html.



   ### Databricks Asset Bundles


   Databricks.yml set targets with a host and root path for each target.
   The host is the workspace URL, and the root path is the folder in the workspace
   where the project will be deployed.

   In test target is the bundle is deployed to a shared folder for testing.
   In dev target, the bundle is deployed to a personal folder for development.



   ### Databricks Auth Type

   .databricks.cfg defines databricks profiles for authentication.
   The default profile uses a PAT token for authentication.
   The token is stored in the DATABRICKS_TOKEN environment variable.

   When you run bundle commands (databricks bundle deploy, databricks bundle run etc.), the DAB tooling uses the profiles in .databrickscfg to authenticate and deploy to the correct workspace.

   ### Databricks Bundle Development 

   Must be deployed to a personal folder in the workspace. Dont't wont to overwrite other developers work.
   The dev target in databricks.yml is set to deploy to a personal folder in the workspace.
   The personal folder is defined by the USER environment variable.

   ### Deploy

   ```
   databricks bundle deploy --target dev
   ```

   However dev is default target, so you can also run

   ```
   databricks bundle deploy
   ```

   Test

   Don't need to specifiy the profile as it will automatically use the profile defined in the target (workspave url).

   ```
   databricks bundle deploy --target test -p TEST
   ```

   In development mode it prefixes all jobs with "[dev USER] " and pauses schedules.

   ### How to deploy Jobs

   Best way (?) create by databricks UI then export to yaml and add to resources folder.
   Then modify the yaml to use parameters for paths etc. Should be defined under the top level key "resources". May have a list of tasks, in addition to a job_cluster

   ### Modularize Configuration Files

   include key (includes other yaml files)


   ### How Databricks Track Changes
   
   Metadata in databricks/bundle/dev keep snapshot of uploaded assets.
   When deploying, compares local files with the snapshot to determine what has changed and only upload changed assets. Only files changes since last deployment are uploaded. Reset state by deleting .databricks directory.


   ### Databricks Connect
   Databricks Connect allows you to connect your favorite IDE to your Databricks cluster. https://docs.databricks.com/dev-tools/databricks-connect.html. Install it in a virtual environment. Install the same version as your cluster runtime. 



   ### External Data 
   - Storage credential: An authentication and authorization object that enables Databricks to access data in external locations. https://docs.databricks.com/data/data-sources/azure/azure-storage-credentials.html
   - External location: An object that combines a storage credential with a cloud storage path. https://docs.databricks.com/data/data-sources/azure/azure-external-locations.html


   ### Secrets
   Store secrets such as database connection strings, passwords, or API keys in secret scopes.
   Creates a "folder" in the Databricks workspace to store secrets.

   ```
   databricks secrets create-scope storage-secrets
   ```

   Add secrets to the scope

   ```
   databricks secrets put-secret storage-secrets --key <key-name>
   ```